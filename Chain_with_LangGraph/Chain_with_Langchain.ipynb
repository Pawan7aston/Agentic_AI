{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 1. **Chat Messages**\n",
    "\n",
    "In LangChain/LangGraph, **chat messages** are the building blocks of a conversation. Instead of just plain text prompts, you structure inputs/outputs as **message objects** with roles.\n",
    "\n",
    "* Types of messages:\n",
    "\n",
    "  * **SystemMessage** → sets context/behavior (“You are a financial assistant…”).\n",
    "  * **HumanMessage** → represents user input.\n",
    "  * **AIMessage** → represents model responses.\n",
    "  * **ToolMessage / FunctionMessage** → represents calls to tools/functions and their outputs.\n",
    "\n",
    "Why important in LangGraph:\n",
    "When you design an agent or chain, you don’t just pass strings — you pass **lists of messages**. This makes conversations stateful, lets you inject tool outputs, and helps maintain multi-turn context.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **Chat Model**\n",
    "\n",
    "A **Chat Model** is simply a wrapper around an LLM (like OpenAI’s GPT, Anthropic’s Claude, etc.) that understands **chat message objects** instead of raw strings.\n",
    "\n",
    "* In LangChain:\n",
    "\n",
    "  ```python\n",
    "  from langchain_openai import ChatOpenAI\n",
    "  model = ChatOpenAI(model=\"gpt-4\")\n",
    "  ```\n",
    "* It takes `messages=[SystemMessage(...), HumanMessage(...)]` as input and returns `AIMessage`.\n",
    "\n",
    "Why important in LangGraph:\n",
    "In a LangGraph chain, the **Chat Model node** is the core “reasoning unit.” It interprets structured messages, decides the next step, and generates output that can either go to the user or trigger a tool.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. **Binding Tools**\n",
    "\n",
    "Binding tools = attaching **functions/APIs** to your chat model so it can call them when needed.\n",
    "\n",
    "* Tools can be:\n",
    "\n",
    "  * A database query executor\n",
    "  * A Python function (e.g., `math`, `search_docs`)\n",
    "  * External API (weather, finance, etc.)\n",
    "\n",
    "* In LangChain:\n",
    "\n",
    "  ```python\n",
    "  from langchain_core.tools import tool\n",
    "\n",
    "  @tool\n",
    "  def add_numbers(a: int, b: int) -> int:\n",
    "      return a + b\n",
    "\n",
    "  bound_model = model.bind_tools([add_numbers])\n",
    "  ```\n",
    "\n",
    "* The model can now return a `ToolCall` message, which LangGraph recognizes and routes to the correct function.\n",
    "\n",
    "Why important in LangGraph:\n",
    "Tools are like **actions your agent can take**. Binding tools is how you give the agent capabilities beyond just text output. Without tool binding, the model is just a chat engine.\n",
    "\n",
    "---\n",
    "\n",
    "### Putting it Together in a LangGraph Chain\n",
    "\n",
    "* **Messages** = structured conversation history.\n",
    "* **Chat Model** = brain that processes messages and decides next step.\n",
    "* **Bound Tools** = actions the brain can take inside the graph.\n",
    "\n",
    "Example flow in LangGraph:\n",
    "\n",
    "1. User sends a **HumanMessage**: \"What’s 5+7?\"\n",
    "2. Chat Model sees it, decides to call tool `add_numbers(5,7)`.\n",
    "3. LangGraph routes this **ToolCall** to the tool.\n",
    "4. Tool returns `12` → becomes a **ToolMessage**.\n",
    "5. Chat Model integrates that result and returns final **AIMessage**: \"The answer is 12.\"\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain Using LangGraph\n",
    "\n",
    "1. How to use Chat messages as our graph state.\n",
    "2. How to use chat models in graph nodes\n",
    "3. How to bind tools to our LLM in chat models\n",
    "4. How to to execute the tools call in our graph nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use Chat messages as our graph state ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Assistant\n",
      "\n",
      "Please tell me how can I help\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: Pawan\n",
      "\n",
      "I want to learn coding\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Assistant\n",
      "\n",
      "Which programming Language you want to learn ?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: Pawan\n",
      "\n",
      "Hey i want to learn Python with LLM model development, explain me in 10 bullet points and make sure gap is proper between them\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage,HumanMessage\n",
    "from pprint import pprint  # This library is nothing but print the messages in better way.\n",
    "messages = [AIMessage(content=f\"Please tell me how can I help\",name=\"Assistant\")]\n",
    "messages.append(HumanMessage(content=f\"I want to learn coding\",name=\"Pawan\"))\n",
    "messages.append(AIMessage(content=f\"Which programming Language you want to learn ?\",name=\"Assistant\"))\n",
    "messages.append(HumanMessage(content=f\"Hey i want to learn Python with LLM model development, explain me in 10 bullet points and make sure gap is proper between them\",name=\"Pawan\"))\n",
    "\n",
    "for mesage in messages:\n",
    "    mesage.pretty_print()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chat Models\n",
    "\n",
    "- How to use chat models in graph nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When Chat model called this is how AIMessage model will call and messag is extracted from the model given below.\n",
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(model = \"qwen/qwen3-32b\")\n",
    "result = llm.invoke(messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'completion_tokens': 723,\n",
       "  'prompt_tokens': 76,\n",
       "  'total_tokens': 799,\n",
       "  'completion_time': 1.529984958,\n",
       "  'prompt_time': 0.003247848,\n",
       "  'queue_time': 2.238738405,\n",
       "  'total_time': 1.533232806},\n",
       " 'model_name': 'qwen/qwen3-32b',\n",
       " 'system_fingerprint': 'fp_5cf921caa2',\n",
       " 'service_tier': 'on_demand',\n",
       " 'finish_reason': 'stop',\n",
       " 'logprobs': None}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.response_metadata"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools\n",
    "\n",
    "- Tools can be integrated with LLM models to interact with external systems, External systems can be API's , third party tools.\n",
    "Whenever a query us asked the model can choose to call the tools and this query is based in the natural language input and this will return an output\n",
    "that matches the tool's schema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(a:int , b:int)->int:\n",
    "    # Here defining everything in docstring as this help LLM to understand what to do in this.\n",
    "    \"\"\"\n",
    "    Add a and b\n",
    "\n",
    "    a(int): first int\n",
    "    b(int): second int\n",
    "\n",
    "    return:\n",
    "        int\n",
    "    \"\"\"\n",
    "    return a+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x14fe82fa0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x14fdfd5e0>, model_name='qwen/qwen3-32b', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Binding tool with LLM\n",
    "llm_with_tools=llm.bind_tools([add])\n",
    "tool_call=llm_with_tools.invoke([HumanMessage(content=f\"Hey what answer for 2 plus 3 ?\",name=\"Pawan\")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'add',\n",
       "  'args': {'a': 2, 'b': 3},\n",
       "  'id': 'z0mzd30f8',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To check how tool call responding , we can check from here given below.\n",
    "tool_call.tool_calls"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 1. **Chat Messages (`AnyMessage`)**\n",
    "\n",
    "* Messages are the **atoms of conversation**.\n",
    "* Types: `HumanMessage`, `AIMessage`, `SystemMessage`, `ToolMessage`…\n",
    "* `AnyMessage` = **union type** → allows a list to store *any* of these messages.\n",
    "* In the graph state we write:\n",
    "\n",
    "  ```python\n",
    "  messages: Annotated[list[AnyMessage], add_messages]\n",
    "  ```\n",
    "\n",
    "  This means:\n",
    "\n",
    "  * `messages` holds a conversation history of mixed roles.\n",
    "  * Every node that produces messages will **append** them (thanks to `add_messages`) instead of overwriting.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **Chat Model**\n",
    "\n",
    "* A `ChatModel` (e.g., `ChatOpenAI`) consumes a list of messages and produces an `AIMessage`.\n",
    "* That `AIMessage` may contain:\n",
    "\n",
    "  * `content` → plain text\n",
    "  * `additional_kwargs` → tool/function calls\n",
    "  * `response_metadata` → token usage, model id, etc.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. **Binding Tools**\n",
    "\n",
    "* We can bind Python functions or APIs as “tools” so the model can call them.\n",
    "* When bound, the model might return an `AIMessage` with a `tool_call`.\n",
    "* The graph routes this to the right function, then logs its output as a `ToolMessage`.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. **Router**\n",
    "\n",
    "* Router inspects the **last message**:\n",
    "\n",
    "  * If it’s a **tool call**, route to tool executor.\n",
    "  * Otherwise, route to final output.\n",
    "* This keeps logic clean, instead of writing nested `if/else`.\n",
    "\n",
    "---\n",
    "\n",
    "# Example: LangGraph Mini Agent\n",
    "\n",
    "Here’s a self-contained demo that shows everything:\n",
    "\n",
    "```python\n",
    "from typing import Annotated, TypedDict\n",
    "from langchain_core.messages import AnyMessage, HumanMessage, AIMessage, ToolMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, add_messages, END\n",
    "\n",
    "# 1. Define State\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "\n",
    "# 2. Define a Tool\n",
    "@tool\n",
    "def add_numbers(a: int, b: int) -> int:\n",
    "    \"\"\"Add two numbers.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "# 3. Chat Model (with bound tool)\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\").bind_tools([add_numbers])\n",
    "\n",
    "# 4. Router function\n",
    "def router(state: State):\n",
    "    last = state[\"messages\"][-1]\n",
    "    if isinstance(last, AIMessage) and last.additional_kwargs.get(\"tool_calls\"):\n",
    "        return \"tool_executor\"\n",
    "    return \"final_output\"\n",
    "\n",
    "# 5. Tool executor node\n",
    "def tool_executor(state: State):\n",
    "    last = state[\"messages\"][-1]\n",
    "    tool_call = last.additional_kwargs[\"tool_calls\"][0]\n",
    "    fn_name = tool_call[\"function\"][\"name\"]\n",
    "    args = eval(tool_call[\"function\"][\"arguments\"])\n",
    "    result = add_numbers(**args)  # call the tool\n",
    "    return {\"messages\": [ToolMessage(content=str(result), tool_call_id=tool_call[\"id\"])]}\n",
    "\n",
    "# 6. Final output node\n",
    "def final_output(state: State):\n",
    "    last = state[\"messages\"][-1]\n",
    "    print(\" Agent says:\", last.content)\n",
    "    return state\n",
    "\n",
    "# 7. Build Graph\n",
    "workflow = StateGraph(State)\n",
    "workflow.add_node(\"model\", model)\n",
    "workflow.add_node(\"tool_executor\", tool_executor)\n",
    "workflow.add_node(\"final_output\", final_output)\n",
    "\n",
    "workflow.set_entry_point(\"model\")\n",
    "workflow.add_conditional_edges(\"model\", router, {\"tool_executor\": \"tool_executor\", \"final_output\": \"final_output\"})\n",
    "workflow.add_edge(\"tool_executor\", \"model\")\n",
    "workflow.add_edge(\"final_output\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "# 8. Run Graph\n",
    "state = {\"messages\": [HumanMessage(content=\"What is 5+7?\")]}\n",
    "final_state = app.invoke(state)\n",
    "\n",
    "print(\"\\n--- Conversation History ---\")\n",
    "for m in final_state[\"messages\"]:\n",
    "    print(type(m).__name__, \":\", m.content)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# How This Works\n",
    "\n",
    "1. **State setup**\n",
    "\n",
    "   * `messages: Annotated[list[AnyMessage], add_messages]`\n",
    "   * Ensures conversation history grows properly.\n",
    "\n",
    "2. **Flow**\n",
    "\n",
    "   * Human asks: `\"What is 5+7?\"`\n",
    "   * Chat model (`AIMessage`) decides to call `add_numbers`.\n",
    "   * Router detects tool call → sends state to `tool_executor`.\n",
    "   * Tool executes and returns `ToolMessage(\"12\")`.\n",
    "   * Model sees tool result, produces final `AIMessage(\"The answer is 12\")`.\n",
    "   * Router detects no tool call → routes to `final_output`.\n",
    "\n",
    "3. **History after run**\n",
    "\n",
    "   ```\n",
    "   HumanMessage : What is 5+7?\n",
    "   AIMessage    :   (tool call request)\n",
    "   ToolMessage  : 12\n",
    "   AIMessage    : The answer is 12\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "# Key Takeaway\n",
    "\n",
    "* **`AnyMessage`**: allows mixed message roles in state. (Human Message and AI message)\n",
    "* **`add_messages` (reducer)**: appends new messages, keeps history intact.\n",
    "* **Chat Model**: generates `AIMessage` (may contain tool calls).\n",
    "* **Router**: decides next step based on the last message.\n",
    "* **Tools**: extend model capabilities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph.message import add_messages  # Reducer , it will merge messages and will not allow override messages , allow only append.\n",
    "from typing import Annotated\n",
    "from langchain_core.messages import AnyMessage\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages:Annotated[list[AnyMessage],add_messages]\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducer with add_messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='Please tell me how may i help you ', additional_kwargs={}, response_metadata={}, name='Assistant'),\n",
       " HumanMessage(content='I want to learn coding', additional_kwargs={}, response_metadata={}, name='Pawan')]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# THis is message feed to LLM model\n",
    "initial_messages = [AIMessage(content=f\"Please tell me how may i help you \", name=\"Assistant\")]\n",
    "initial_messages.append(HumanMessage(content=f\"I want to learn coding\",name=\"Pawan\"))\n",
    "initial_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Which programming language you want to learn', additional_kwargs={}, response_metadata={}, name='Assistant')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is another set of AI message now will feed this to LLM model , by appending this to initial_meassge also this will not override previous message\n",
    "ai_message=AIMessage(content=f\"Which programming language you want to learn\",name=\"Assistant\")\n",
    "ai_message"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Role of Reducer , to append instead of override"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='Please tell me how may i help you ', additional_kwargs={}, response_metadata={}, name='Assistant', id='5946bffc-2b1f-416c-a5b4-b710465665b5'),\n",
       " HumanMessage(content='I want to learn coding', additional_kwargs={}, response_metadata={}, name='Pawan', id='fbc86e05-278d-4bd8-9a09-54ff89e99a29'),\n",
       " AIMessage(content='Which programming language you want to learn', additional_kwargs={}, response_metadata={}, name='Assistant', id='cd249376-b01b-4a46-81d7-70489fc8000e')]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here i added initial_messages with ai_message and here we can see initial and AI message is combined together\n",
    "add_messages(initial_messages,ai_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chatbot node functionality \n",
    "def llm_tool(state:State):\n",
    "    return {\"messages\":[llm_with_tools.invoke(state[\"messages\"])]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x14fe82f40>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image , display\n",
    "from langgraph.graph import StateGraph , START,END\n",
    "builder = StateGraph(State)\n",
    "\n",
    "builder.add_node(\"llm_tool\",llm_tool)\n",
    "builder.add_edge(START,\"llm_tool\")\n",
    "builder.add_edge(\"llm_tool\",END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is 2 plus 7\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  add (j2mhma3qs)\n",
      " Call ID: j2mhma3qs\n",
      "  Args:\n",
      "    a: 2\n",
      "    b: 7\n"
     ]
    }
   ],
   "source": [
    "# Invocation\n",
    "\n",
    "messages = graph.invoke({\"messages\":\"What is 2 plus 7\"})\n",
    "\n",
    "for message in messages[\"messages\"]:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
